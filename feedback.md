<head>
	<script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>

# Initial feedback from Amali
The table shows a brief overview over the feedback from Amali, the full text can be seen below the table.  

:white_check_mark: indicates a completed task.

:warning: indicates a task where more information is needed

:white_large_square: indicates a task that have not been started on yet.

| No.   | Description   																| Done  					|
|:-----:|:------------------------------------------------------------------------------|:-------------------------:|
|	1a	| Informal language				 												|	:white_large_square:	|
|	1b	| Experimental details were described in story format. 							|	:white_large_square:	|
|	2	| Add more details about simulations, and more screenshots. 					|	:white_check_mark:	|
|	3	| Include a table to explain the procedure for both groups. Section 14.1 14.2 	|	:white_large_square:	|
|	4	| Justification of using the LOGO language is too weak.							|	:white_check_mark:	|
|	5	| Results: compate the pre-test of both groups, then compare the improvements.	|	:white_check_mark:		|
|	6	| Results: Use `gain` instead of `diff`											|	:white_check_mark:		|
|	7	| Calculate normalized gains in results 										|	:white_check_mark:		|
|	8	| Details for which tasks were completed by each group.							|	:warning:				|
|	9	| Include a table of statistical details, see table 1 in atteched paper. 		|	:warning:				|
|	10	| Did you give partial marks in the pre- and post-test?							|	:white_check_mark:	|
|	11	| Be very clear about the role the teacher had in designing the experiment.		|	:white_large_square:	|
|	12	| Ch:observations:: include introductions to the different categories			|	:white_large_square:	|
|	13	| Ch:observations:: analyze means of each category								|	:white_large_square:	|
|	14	| Ch:observations:: clearify if partial marks were given						|	:white_check_mark:	|
|	15	| Ch:observations:: Marking of explanations, analysis of explanations?			|	:white_chack_mark:	|


# Comments to tasks

**2:** Some of this was added in one of the appendices after the first draft was sent to Amali. But we should probably try to merge most of it into the report, perhaps even keep the appendix as a duplicate.

**8:** Do we still have these results? Or did we throw them away?

**9:** Tried consolidating some of the results in the `Result` chapter. I do believe most of the results are there, but should probably be presented in a cleaner way.

**13-15:** Amali would like the results from these points i a table

**other:** Q1, Q2, and Q3 (she probably means the tasks numbers on the pretest) are testing declarative knowledge. All the other require applying declarative knowledge (i.e. procedural knowledge). This way we can see if wether students improved in declarative or procedural knowledge.

# Keypoints from the feedback

1. Informal language
2. Experiment details were described in a story format.	
	* e.g "The first session was delayed because we were waiting for an extra tablet which never showed up" should rather be "We had to use phone instead of tablets due to unavailability." Then discuss the effect of this on their learning experience
	* e.g "we arrived at the school at 9.00.. Then the students were already in the class..." The important thing here is the fact that: This is the first learning activity that the students did on the day.  This may have a positive effect on their learning. Was it the case for both groups?
3. It would also be good to include a table to explain the procedure for both groups (what is discussed in 14.1 and 14.2). It is not clear who did what e.g. who did the introduction and who did the recap?
4. Justification of using the LOGO programming language is weak. 
5. When you present the results, first you need to compare the pre-test knowledge of both groups. Then compare the improvement in performance.
6. Rather then say Diff, it would be good to use the word 'gain' or 'learning gain'
7. it would be good if you could also calculate the normalise gain, and compare it between the groups. Normalised learning $gain_{normalized} = \frac{gain}{100-pretest}.$. This will explain the gain in relation to how much room they have for improvement. for e.g. if some one scored 75/100 in the pre-test, the room for improvement is 25 and the post-test score is 85. normalised gain = 10/25.
8. Do you have details of which tasks were completed by each group for each session? would be good to find out the mean number of tasks completed? 
9. A table that gives statistical details will improve the readability. for e.g. this table should have number of students for each group, mean pre-test scores , post-test scores, mean number of tasks completed etc. Please refer to the table 1 in the paper attached. 
10. Did you give partial marks in the pre- and post-tests ? 
11. One of the important aspects of this study was that we got the teachers involved and got ideas from them how they teach maths etc. Rather than just going to school and did the experiment. So it needs to be clearly explained.
12. Chapter 23(observations, I believe this is now chapter 25) is hard to follow.  
	It's easier if you introduce some sort of categorisation to the pre-test.e.g  
	1: Definitions of angles  
	2, 3: Drawing angles  
	4: Comparing the size of an angle  
	5: Find the mission angle  
	6, 7: Estimating the size of the angles.   
	etc  
13. Analysis of observations
	1. you can compare the means mark for each category (Where does this fit in our report? Additional analysis perhaps?)
	2. Did we mentioned if we gave partial marks, worth mentioning in the report.
	3. Did you mark the explanations? If so, you can calculate the mean score for each calculation. This way you can compare whether the quality of the explanations improved.
	4. The result would look good in a table.
14. If scoring to Q1, Q2, Q3 are applied we can test the difference between declarative and procedural knowledge. 

# The first mail from Amali

```text
Hi Nicklas, Jan 

Thanks Nicklas. I've gone through the previous draft. Apologies for the delay in sending feedback, have been pretty busy with work.

You seem to have included all the details in the thesis. 

(i) However the language used is informal. As it is a professional document, the language should be formal. The other thing is the details are given in a story format specially when the experiment details are described. 

for e.g. The first session was delayed because we were waiting for an extra tablet which never showed up...

Instead you could have said:We had to use phone instead of tablets due to unavailability. Then discuss the effect of this on their learning experience.

Another e.g. is "we arrived at the school at 9.00.. Then the students were already in the class..."
The important thing here is the fact that: This is the first learning activity that the students did on the day.  This may have a positive effect on their learning. Was it the case for both groups>?

(ii) It would be great if you could include more details about the simulation. Also please include screen shots to improve the readability. 

(iii) It would also be good to include a table to explain the procedure for both groups (what is discussed in 14.1 and 14.2). It is not clear who did what e.g. who did the introduction and who did the recap?
(iv) Justification of using the LOGO programming language is weak. 
(v) When you present the results, first you need to compare the pre-test knowledge of both groups. Then compare the improvement in performance.
(vi) Rather then say Diff, it would be good to use the word 'gain' or 'learning gain'
(vii) it would be good if you could also calculate the normalise gain, and compare it between the groups. Normalised learning gain =learning gain/(100-pre-test score). This will explain the gain in relation to how much room they have for improvement. for e.g. if some one scored 75/100 in the pre-test, the room for improvement is 25 and the post-test score is 85. normalised gain = 10/25.
(viii) Do you have details of which tasks were completed by each group for each session? would be good to find out the mean number of tasks completed? 
(viii) A table that gives statistical details will improve the readability. for e.g. this table should have number of students for each group, mean pre-test scores , post-test scores, mean number of tasks completed etc. Please refer to the table 1 in the paper attached. 
(ix) Did you give partial marks in the pre- and post-tests ? 
(x) One of the important aspects of this study was that we got the teachers involved and got ideas from them how they teach maths etc. Rather than just going to school and did the experiment. So it needs to be clearly explained.


I'll send some more feedback about the last two chapters, as I'm still thinking of a good way to analyse the questions in the pre-and post-tests in a  detailed way. 

Hope this helps... Please feel to ask further questions if feedback is not clear.

Thanks

Amali

```

# The second mail from Amali
```text
Hi Nicklas, Jan

Hope you got my previous email. How's the writing going ?

Some ideas to discuss the answers in more detail (chapter 23). This chapter is a hard to follow.

It's easier if you introduce some sort of categorisation to the pre-test

1. Definition of angles
2,3 - drawing angles  
4 - Comparing the size of an angle
5 Find the missing angle
6,7 - estimating the size of the angles etc.

Now some things you can do in terms of analysis

1. You can compare the mean mark for each category
2. It was not clear whether you gave partial marks, worth mentioning in the report
3. Did you mark the explanations? If so, you can calculate the mean score for each calculation. This way you can compare whether the qaulity of the explanations improved. 

It would be good if you can present them in a  table. 

Q1,2 and 3 are testing declarative knowledge because they are asking students to write text-book definitions. All the other questions require applying declarative knowledge (i.e. procedural knowledge).  Then we can see whether student improvement was in the declarative or procedural knowledge or both (if you calculate the mean scores for each of these categories).

Thanks

Amali

```
